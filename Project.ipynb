{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(split_name='train', columns=['text', 'label'], folder='data'):\n",
    "    '''\n",
    "        \"split_name\" may be set as 'train', 'valid' or 'test' to load the corresponding dataset.\n",
    "        \n",
    "        You may also specify the column names to load any columns in the .csv data file.\n",
    "        Among many, \"text\" can be used as model input, and \"label\" column is the labels (sentiment). \n",
    "    '''\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"Success\")\n",
    "        return df\n",
    "    except:\n",
    "        print(f\"Failed loading specified columns... Returning all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, label] columns from the train split\n",
      "Success\n",
      "select [text, label] columns from the valid split\n",
      "Success\n",
      "select [id, text] columns from the test_no_label split\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text', 'label'], folder='data')\n",
    "valid_df = load_data('valid', columns=['text', 'label'], folder='data')\n",
    "# the test set labels (the 'label' column) are unavailable! So the following code will instead return all columns\n",
    "test_df = load_data('test_no_label', columns=['id', 'text'], folder='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two Wolfgang Petersen directed films together ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For fans of the series and the movies\\nthis fi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the movie. The Blu-ray was fine, but it...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You don't know what is going on until the end ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We only watched a few minutes of the movie, du...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Two Wolfgang Petersen directed films together ...      5\n",
       "1  For fans of the series and the movies\\nthis fi...      4\n",
       "2  I love the movie. The Blu-ray was fine, but it...      3\n",
       "3  You don't know what is going on until the end ...      3\n",
       "4  We only watched a few minutes of the movie, du...      1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lower(s):\n",
    "    \"\"\"\n",
    "    :param s: a string.\n",
    "    return a string with lower characters\n",
    "    Note that we allow the input to be nested string of a list.\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: 'text mining is to identify useful information.'\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return [lower(t) for t in s]\n",
    "    if isinstance(s, str):\n",
    "        return s.lower()\n",
    "    else:\n",
    "        raise NotImplementedError(\"unknown datatype\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "# Perform one-hot\n",
    "# e.g \n",
    "# texts = [\n",
    "#    [\"apple\", \"banana\", \"orange\"],\n",
    "#    [\"orange\", \"apple\"],\n",
    "#    [\"banana\", \"banana\"],\n",
    "#    [\"apple\", \"orange\"],\n",
    "#    [\"banana\", \"apple\", \"orange\"]\n",
    "#]\n",
    "\n",
    "# feats_dict = {'apple': 0, 'banana': 1, 'orange': 2}\n",
    "\n",
    "# feature_vectors = []\n",
    "#for text in texts:\n",
    "#    onehot_vector = get_onehot_vector(text, feats_dict)\n",
    "#    feature_vectors.append(onehot_vector)\n",
    "\n",
    "# feature_vectors\n",
    "#feature_vectors = [\n",
    "#    [1, 1, 1],\n",
    "#    [1, 0, 1],\n",
    "#    [0, 1, 0],\n",
    "#    [1, 0, 1],\n",
    "#    [1, 1, 1]\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list(list)\n",
    "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
    "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
    "    :param max_size: the max size of feature dict, type: int\n",
    "    return a feature dict that maps features to indices, sorted by frequencies\n",
    "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
    "    \"\"\"\n",
    "    # count all features\n",
    "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
    "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
    "        valid_feats = [\"<pad>\", \"<unk>\"] + [f for f, cnt in feat_cnt.most_common(max_size-2)]\n",
    "    else:\n",
    "        valid_feats = [\"<pad>\", \"<unk>\"]\n",
    "        for f, cnt in feat_cnt.most_common():\n",
    "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
    "                (max_freq == -1 or cnt <= max_freq):\n",
    "                valid_feats.append(f)\n",
    "    if max_size > 0 and len(valid_feats) > max_size:\n",
    "        valid_feats = valid_feats[:max_size]\n",
    "    print(\"Size of features:\", len(valid_feats))\n",
    "\n",
    "    # build a mapping from features to indices\n",
    "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
    "    return feats_dict\n",
    "\n",
    "def get_index_vector(feats, feats_dict, max_len):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    :param feats: a list of features, type: list\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(max_len, dtype=np.int64)\n",
    "    for i, f in enumerate(feats):\n",
    "        if i == max_len:\n",
    "            break\n",
    "        # get the feature index, return 1 (<unk>) if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, 1)\n",
    "        vector[i] = f_idx\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreProcessing (Token and stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, label] columns from the train split\n",
      "Success\n",
      "select [text, label] columns from the valid split\n",
      "Success\n",
      "                                                text  label\n",
      "0  Two Wolfgang Petersen directed films together ...      5\n",
      "1  For fans of the series and the movies\\nthis fi...      4\n",
      "2  I love the movie. The Blu-ray was fine, but it...      3\n",
      "3  You don't know what is going on until the end ...      3\n",
      "4  We only watched a few minutes of the movie, du...      1\n",
      "[['two wolfgang', 'wolfgang petersen', 'petersen direct', 'direct film', 'film togeth', 'togeth one', 'one packag', 'packag could', 'could want', 'want ,', ', fine', 'fine pictur', 'pictur qualiti', 'qualiti extra', 'extra old', 'old dvd', 'dvd .', '. ``', '`` air', 'air forc', 'forc one', \"one ''\", \"'' particular\", 'particular excel', 'excel .'], ['fan seri', 'seri movi', 'movi thi', 'thi film', 'film must', 'must .', '. continu', 'continu wrath', 'wrath khan', 'khan level', 'level interest', 'interest .', '. anyway', 'anyway good', 'good movi'], ['love movi', 'movi .', '. blu-ray', 'blu-ray wa', 'wa fine', 'fine ,', ', came', 'came expir', 'expir digit', 'digit copi', 'copi code', 'code .', '. wa', 'wa disappoint'], [\"n't know\", 'know go', 'go end', 'end movi', 'movi .', '. seen', 'seen movi', \"movi n't\", \"n't know\", 'know happen', 'happen end', 'end thi', 'thi leav', 'leav cold', 'cold .'], ['onli watch', 'watch minut', 'minut movi', 'movi ,', ', due', 'due offens', 'offens content', 'content .', '. want', 'want see', 'see ,', ', becaus', 'becaus wee', 'wee heard', 'heard mani', 'mani time', 'time .']]\n",
      "[['two wolfgang petersen', 'wolfgang petersen direct', 'petersen direct film', 'direct film togeth', 'film togeth one', 'togeth one packag', 'one packag could', 'packag could want', 'could want ,', 'want , fine', ', fine pictur', 'fine pictur qualiti', 'pictur qualiti extra', 'qualiti extra old', 'extra old dvd', 'old dvd .', 'dvd . ``', '. `` air', '`` air forc', 'air forc one', \"forc one ''\", \"one '' particular\", \"'' particular excel\", 'particular excel .'], ['fan seri movi', 'seri movi thi', 'movi thi film', 'thi film must', 'film must .', 'must . continu', '. continu wrath', 'continu wrath khan', 'wrath khan level', 'khan level interest', 'level interest .', 'interest . anyway', '. anyway good', 'anyway good movi'], ['love movi .', 'movi . blu-ray', '. blu-ray wa', 'blu-ray wa fine', 'wa fine ,', 'fine , came', ', came expir', 'came expir digit', 'expir digit copi', 'digit copi code', 'copi code .', 'code . wa', '. wa disappoint'], [\"n't know go\", 'know go end', 'go end movi', 'end movi .', 'movi . seen', '. seen movi', \"seen movi n't\", \"movi n't know\", \"n't know happen\", 'know happen end', 'happen end thi', 'end thi leav', 'thi leav cold', 'leav cold .'], ['onli watch minut', 'watch minut movi', 'minut movi ,', 'movi , due', ', due offens', 'due offens content', 'offens content .', 'content . want', '. want see', 'want see ,', 'see , becaus', ', becaus wee', 'becaus wee heard', 'wee heard mani', 'heard mani time', 'mani time .']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "train_df = load_data('train')[:6000]\n",
    "valid_df = load_data('valid')\n",
    "x_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "x_valid = valid_df['text']\n",
    "y_valid = valid_df['label']\n",
    "train_tokens = [tokenize(text) for text in x_train]\n",
    "test_tokens = [tokenize(text) for text in x_valid]\n",
    "\n",
    "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
    "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
    "\n",
    "train_stemmed = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
    "test_stemmed = [filter_stopwords(tokens) for tokens in test_stemmed]\n",
    "\n",
    "train_2_gram = [n_gram(tokens, 2) for tokens in train_stemmed]\n",
    "train_3_gram = [n_gram(tokens, 3) for tokens in train_stemmed]\n",
    "test_2_gram = [n_gram(tokens, 2) for tokens in test_stemmed]\n",
    "test_3_gram = [n_gram(tokens, 3) for tokens in test_stemmed]\n",
    "\n",
    "print(train_2_gram[:5])\n",
    "print(train_3_gram[:5])\n",
    "train_2_gram = train_stemmed\n",
    "test_2_gram = test_stemmed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Learning.( just copy first) \\\\ RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size,\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=False,\n",
    "              embedding_matrix=None,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_rnn_layers: the number of layers of the RNN, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param rnn_type: the type of RNN, type: str\n",
    "    :param bidirectional: whether to use bidirectional rnn, type: bool\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a RNN for text classification,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # recurrent layers document: https://keras.io/layers/recurrent\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "\n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    if embedding_matrix is not None:\n",
    "        # TODO\n",
    "        pass\n",
    "    else:\n",
    "        emb = Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_size,\n",
    "                        input_length=input_length,\n",
    "                        embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "\n",
    "    ################################\n",
    "    ####### Recurrent Layers #######\n",
    "    ################################\n",
    "    # recurrent layers\n",
    "    if rnn_type == \"rnn\":\n",
    "        fn = SimpleRNN\n",
    "    elif rnn_type == \"lstm\":\n",
    "        fn = LSTM\n",
    "    elif rnn_type == \"gru\":\n",
    "        fn = GRU\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    h = emb\n",
    "    for i in range(num_rnn_layers):\n",
    "        is_last = (i == num_rnn_layers-1)\n",
    "        if bidirectional:\n",
    "            # TODO\n",
    "            h = keras.layers.Bidirectional(fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=not is_last))(h)\n",
    "        else:\n",
    "            h = fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=not is_last)(h)\n",
    "        h = Dropout(dropout_rate, seed=0)(h)\n",
    "\n",
    "    ################################\n",
    "    #### Fully Connected Layers ####\n",
    "    ################################\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                      bias_initializer=\"zeros\",\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add residual connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = Dense(output_size,\n",
    "              activation=\"softmax\",\n",
    "              kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "              bias_initializer=\"zeros\")(h)\n",
    "\n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\anacondom\\envs\\fl\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Input, Add, Concatenate,\\\n",
    "    Bidirectional, SimpleRNN, LSTM, GRU\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of features: 2918\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "feats_dict = get_feats_dict(\n",
    "    chain.from_iterable(train_2_gram),\n",
    "    min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "\n",
    "# build the feats_matrix\n",
    "# convert each example to a index vector, and then stack vectors as a matrix\n",
    "train_feats_matrix = np.vstack(\n",
    "    [get_index_vector(f, feats_dict, max_len) for f in train_2_gram])\n",
    "test_feats_matrix = np.vstack(\n",
    "    [get_index_vector(f, feats_dict, max_len) for f in test_2_gram])\n",
    "\n",
    "# convert labels to label_matrix\n",
    "num_classes = max(y_train)\n",
    "# convert each label to a ont-hot vector, and then stack vectors as a matrix\n",
    "train_label_matrix = keras.utils.to_categorical(y_train-1, num_classes=num_classes)\n",
    "test_label_matrix = keras.utils.to_categorical(y_valid-1, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 3s 22ms/step - loss: 2.5428 - accuracy: 0.2691 - val_loss: 2.3035 - val_accuracy: 0.3450\n",
      "Epoch 2/100\n",
      " 9/54 [====>.........................] - ETA: 0s - loss: 2.2005 - accuracy: 0.3822"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anacondom\\envs\\fl\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 14ms/step - loss: 2.0546 - accuracy: 0.4209 - val_loss: 2.0168 - val_accuracy: 0.4150\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 1.6953 - accuracy: 0.5200 - val_loss: 1.8164 - val_accuracy: 0.4417\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 1.4400 - accuracy: 0.5848 - val_loss: 1.6777 - val_accuracy: 0.4600\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 1.2538 - accuracy: 0.6309 - val_loss: 1.5824 - val_accuracy: 0.4767\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 1.0840 - accuracy: 0.6922 - val_loss: 1.5812 - val_accuracy: 0.4467\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.9446 - accuracy: 0.7287 - val_loss: 1.5840 - val_accuracy: 0.4633\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.8164 - accuracy: 0.7687 - val_loss: 1.6466 - val_accuracy: 0.4250\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.7066 - accuracy: 0.7956 - val_loss: 1.7618 - val_accuracy: 0.4583\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.6191 - accuracy: 0.8191 - val_loss: 1.9909 - val_accuracy: 0.4333\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 0.5493 - accuracy: 0.8424 - val_loss: 2.1893 - val_accuracy: 0.4467\n",
      "60/60 [==============================] - 1s 6ms/step - loss: 1.3184 - accuracy: 0.6610\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5903 - accuracy: 0.4550\n",
      "training loss: 1.3183810710906982 training accuracy 0.6610000133514404\n",
      "test loss: 1.5903183221817017 test accuracy 0.45500001311302185\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "# TODO\n",
    "model = build_RNN(input_length=max_len,\n",
    "                  vocab_size=len(feats_dict),\n",
    "                  embedding_size=100,\n",
    "                  hidden_size=100,\n",
    "                  output_size=num_classes,\n",
    "                  num_rnn_layers=1,\n",
    "                  num_mlp_layers=2,\n",
    "                  rnn_type=\"lstm\",\n",
    "                  bidirectional=False,\n",
    "                  embedding_matrix=None,\n",
    "                  activation=\"tanh\",\n",
    "                  dropout_rate=0.5,\n",
    "                  l2_reg=0.005,\n",
    "                  batch_norm=True,\n",
    "                  )\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "earlystopping =  keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
