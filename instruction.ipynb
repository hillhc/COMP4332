{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u-Pk-Xb5UJH"
   },
   "source": [
    "# Instructions for Project 1 - Sentiment Analysis\n",
    "\n",
    "Hello everyone, this is Zhaowei Wang. I am glad to host the first project. My email is *zwanggy@connect.ust.hk*. Feel free to send me an email if you have any problem regarding this project.\n",
    "\n",
    "In this project, you will try to work on a sentiment analysis task.\n",
    "You will build a model to predict the scores (a.k.a. the \"label\" column in datasets, from 1 to 5) of each review.\n",
    "For each review, you are given a piece of text. You can consider the predicted variables as categorical, ordinal or numerical.\n",
    "\n",
    "Just a kind note: The codes and techniques introduced in the previous tutorials may come in handy. You can refer to the .ipynb notebooks for details.\n",
    "\n",
    "## Important dates, submission requirements and grading policy \n",
    "**Important dates:**\n",
    "- *March 16, 2024 (Saturday)*: Project starts\n",
    "- *March 23, 2024 (Saturday)* Release the validation score of baselines\n",
    "- *April 6, 2024, 23:59 (Saturday)*: `Submission Deadline`\n",
    "\n",
    "**Submission requirements:**  \n",
    "Each team leader is required to submit the groupNo.zip file on the Canvas. It shoud contain \n",
    "- `pred.csv`: Predictions on test data (please make sure you can successfully evaluate your validation predictions on the validation data with the help of evaluate.py). The file should contain two so-called columns, which are `id`\n",
    "and `label`.\n",
    "- report (1-2 pages of pdf)\n",
    "- code (Frameworks and programming languages are not restricted)\n",
    "\n",
    "**Grading policy:**  \n",
    "We will check your report with your code and your model performance (in terms of Accuracy) on the test set.\n",
    "\n",
    "| Grade | Classifier (80%)                                                   | Report (20%)                      |\n",
    "|-------|--------------------------------------------------------------------|-----------------------------------|\n",
    "| 50%   | Example code in tutorials or in Project 1 without any modification | submission                        |\n",
    "| 75%   | A method that can outperform the easy baseline  | algorithm you used                |\n",
    "| 95%   | A method that can outperform the hard baseline                     | detailed explanation and analysis, such as explorative data analysis, hyperparameters and ablation studies  |\n",
    "| 100%  | A method that can outperform the hard baseline with at least one excellent idea  | excellent ideas, detailed explanation and solid analysis |\n",
    "\n",
    "## Instruction Content\n",
    "In this notebook, you are provided with the code snippets to start with.\n",
    "\n",
    "The content follows previous lectures and tutorials. But some potentially useful python packages are also mentioned.\n",
    "\n",
    "1. Loading data and saving predictions\n",
    "    1. Loading data\n",
    "    1. Saving predictions to file\n",
    "1. Preprocessing\n",
    "    1. Text data processing recap\n",
    "    1. Explorative data analysis\n",
    "1. Learning Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpoRui_47pOQ"
   },
   "source": [
    "## 1. Loading data and saving predictions\n",
    "\n",
    "The same as previous tutorials, we use `pandas` as the basic tool to load & dump the data.\n",
    "The key ingredient of our operation is the `DataFrame` in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1678609678007,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "56Yveell5UJM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V9e5rcOs77K9"
   },
   "outputs": [],
   "source": [
    "# if you use Google Colab, un-comment this cell, modify `path_to_data` if needed, and run to mount data to `data`\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# path_to_data = '/content/drive/MyDrive/HKUST stuff/COMP4332_Project1/data'\n",
    "# !rm -f data\n",
    "# !ln -s '/content/drive/MyDrive/HKUST stuff/COMP4332_Project1/data' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo8Z7VVq5UJO"
   },
   "source": [
    "### A. Loading data\n",
    "\n",
    "The following code shows how to load the datasets for this project.  \n",
    "Among which, we do not release the labels (the \"label\" column) for the test set. \n",
    "You may evaluate your trained model on the validation set instead.\n",
    "However, your submitted predictions (``pred.csv``) should be generated on the test set.\n",
    "\n",
    "Each year we release different data, so old models are not guaranteed to solve the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1678609870255,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "coz-oCwS5UJP"
   },
   "outputs": [],
   "source": [
    "def load_data(split_name='train', columns=['text', 'label'], folder='data'):\n",
    "    '''\n",
    "        \"split_name\" may be set as 'train', 'valid' or 'test' to load the corresponding dataset.\n",
    "        \n",
    "        You may also specify the column names to load any columns in the .csv data file.\n",
    "        Among many, \"text\" can be used as model input, and \"label\" column is the labels (sentiment). \n",
    "    '''\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"Success\")\n",
    "        return df\n",
    "    except:\n",
    "        print(f\"Failed loading specified columns... Returning all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJnx8k_s5UJQ"
   },
   "source": [
    "Then you can extract the data by specifying the desired split and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3087,
     "status": "ok",
     "timestamp": 1678609875903,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "UEJmxylT5UJR",
    "outputId": "599a719c-2f0b-425b-a76f-26f5a2aae895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, label] columns from the train split\n",
      "Success\n",
      "select [id, text, label] columns from the valid split\n",
      "Success\n",
      "select [id, text] columns from the test_no_label split\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text', 'label'], folder='data')\n",
    "valid_df = load_data('valid', columns=['id','text', 'label'], folder='data')\n",
    "# the test set labels (the 'label' column) are unavailable! So the following code will instead return all columns\n",
    "test_df = load_data('test_no_label', columns=['id', 'text'], folder='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1678609899213,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "TuA-yimg5UJS",
    "outputId": "81bca1db-177c-4182-9d8d-a19310decbbe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two Wolfgang Petersen directed films together ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For fans of the series and the movies\\nthis fi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the movie. The Blu-ray was fine, but it...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You don't know what is going on until the end ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We only watched a few minutes of the movie, du...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Two Wolfgang Petersen directed films together ...      5\n",
       "1  For fans of the series and the movies\\nthis fi...      4\n",
       "2  I love the movie. The Blu-ray was fine, but it...      3\n",
       "3  You don't know what is going on until the end ...      3\n",
       "4  We only watched a few minutes of the movie, du...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1678609946432,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "ZKc0Sllc5UJS",
    "outputId": "e6ec52bc-726f-450d-ad2f-131193086481"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3EMGD8RAEOK64_2907</td>\n",
       "      <td>On our trip this past summer to Lunenberg, Nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2BOWU2PX28BET_5501</td>\n",
       "      <td>Excellent!! Most remakes fall short of the ori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A100WO06OQR8BQ_10469</td>\n",
       "      <td>I started to watch this movie but it is such a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2H4LKU7CPIUU9_11364</td>\n",
       "      <td>Well! I must be terribly jaded. Or I am comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A14RF11JYGDKI8_23751</td>\n",
       "      <td>Dark and grim -- not a fun movie.  Watch it fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text\n",
       "0   A3EMGD8RAEOK64_2907  On our trip this past summer to Lunenberg, Nov...\n",
       "1   A2BOWU2PX28BET_5501  Excellent!! Most remakes fall short of the ori...\n",
       "2  A100WO06OQR8BQ_10469  I started to watch this movie but it is such a...\n",
       "3  A2H4LKU7CPIUU9_11364  Well! I must be terribly jaded. Or I am comple...\n",
       "4  A14RF11JYGDKI8_23751  Dark and grim -- not a fun movie.  Watch it fo..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1678610237266,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "LFfewsiX5UJT",
    "outputId": "20c456e7-2f7b-4844-9fdd-5b9a0d6bb389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 2000 4000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(valid_df), len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mef_BG-X5UJT"
   },
   "source": [
    "### B. Saving predictions to file\n",
    "\n",
    "Your submitted predictions are supposed to be a .csv file containing two columns, i.e. (``id`` and ``label``). \n",
    "\n",
    "Here, as an example, we generate some random predictions as our answer, which are put in a DataFrame and output to a .csv file\n",
    "\n",
    "After getting your model predictions on the test set, you may follow these steps to generate your ``pred.csv`` file. (By replacing the random predictions with your model predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1678610430357,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "5AmFZ96x5UJU"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678610430934,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "HwYfkZvU5UJU"
   },
   "outputs": [],
   "source": [
    "random_pred = pd.DataFrame(data={\n",
    "    'id': test_df['id'],\n",
    "    'label': np.random.randint(0, 6, size=len(test_df))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1678610431694,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "jGGl8Kt85UJU",
    "outputId": "72ec1c3d-6a0d-4daf-b9af-849a7a1e8773"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3EMGD8RAEOK64_2907</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2BOWU2PX28BET_5501</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A100WO06OQR8BQ_10469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2H4LKU7CPIUU9_11364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A14RF11JYGDKI8_23751</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  label\n",
       "0   A3EMGD8RAEOK64_2907      5\n",
       "1   A2BOWU2PX28BET_5501      5\n",
       "2  A100WO06OQR8BQ_10469      0\n",
       "3  A2H4LKU7CPIUU9_11364      1\n",
       "4  A14RF11JYGDKI8_23751      2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1678610434485,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "XMBpLyd35UJU"
   },
   "outputs": [],
   "source": [
    "random_pred.to_csv(f'random_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb1ZhmEZ5UJV"
   },
   "source": [
    "Then, you will get a ``random_pred.csv`` in your folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7lkS6sqAIT0"
   },
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Here are some preprocessing examples for your reference. For more details you may refer to the previous tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luZkHLq35UJV"
   },
   "source": [
    "### A. Text data processing recap\n",
    "In the tutorials, we have shown how to extract textual features using the `nltk` package\n",
    "\n",
    "Remember to use the NLTK Downloader to obtain the resource first:\n",
    "```\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('stopwords')\n",
    "  >>> nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1678610498614,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "a-KE25vc5UJV"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lower(s):\n",
    "    \"\"\"\n",
    "    :param s: a string.\n",
    "    return a string with lower characters\n",
    "    Note that we allow the input to be nested string of a list.\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: 'text mining is to identify useful information.'\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return [lower(t) for t in s]\n",
    "    if isinstance(s, str):\n",
    "        return s.lower()\n",
    "    else:\n",
    "        raise NotImplementedError(\"unknown datatype\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeAOeeYC5UJW"
   },
   "source": [
    "Note that you can use the `map` function to apply your preprocessing functions into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_df)):\n",
    "    try:\n",
    "        tokenize(test_df.loc[i, 'text'])\n",
    "    except: \n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                     A8NQVLIE0QVT4_7949\n",
      "text    Great movie, even better dubb. Blu ray is the ...\n",
      "Name: 1155, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test_df.loc[1155])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6268,
     "status": "ok",
     "timestamp": 1678610507492,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "hei6VERQ5UJW",
    "outputId": "94858015-82ba-4203-e52d-2d7e68a7c8bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [on, trip, past, summer, lunenberg, ,, nova, s...\n",
      "1    [excellent, !, !, most, remakes, fall, short, ...\n",
      "2    [i, started, watch, movie, lousy, movie, i, st...\n",
      "3    [well, !, i, must, terribly, jaded, ., or, i, ...\n",
      "4    [dark, grim, --, fun, movie, ., watch, perform...\n"
     ]
    }
   ],
   "source": [
    "test_df['tokens'] = test_df['text'].map(tokenize).map(filter_stopwords).map(lower)\n",
    "print(test_df['tokens'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2aF74qi5UJW"
   },
   "source": [
    "Besides `nltk`, `SpaCy` may also be useful.\n",
    "\n",
    "You can explore it at https://spacy.io/\n",
    "\n",
    "Let's install it with the following command (in terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTFX-QsE5UJX"
   },
   "source": [
    "```bash\n",
    "python -m pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCLmErA45UJX"
   },
   "source": [
    "You may use spacy to extract linguistic features from texts\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1678610605522,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "kZz4vWC85UJX",
    "outputId": "0864667b-3dab-46c2-ca4b-83761fad340f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw       ,\t stem      ,\t PartOfSpeech,\t dependency,\t shape     ,\t is alpha  ,\t is stop   ,\t its childrens in the parsing tree,\t \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Apple     ,\t Apple     ,\t PROPN     ,\t nsubj     ,\t Xxxxx     ,\t True      ,\t False     ,\t []        ,\t \n",
      "is        ,\t be        ,\t AUX       ,\t aux       ,\t xx        ,\t True      ,\t True      ,\t []        ,\t \n",
      "looking   ,\t look      ,\t VERB      ,\t ROOT      ,\t xxxx      ,\t True      ,\t False     ,\t [Apple, is, at, startup],\t \n",
      "at        ,\t at        ,\t ADP       ,\t prep      ,\t xx        ,\t True      ,\t True      ,\t [buying]  ,\t \n",
      "buying    ,\t buy       ,\t VERB      ,\t pcomp     ,\t xxxx      ,\t True      ,\t False     ,\t [U.K.]    ,\t \n",
      "U.K.      ,\t U.K.      ,\t PROPN     ,\t dobj      ,\t X.X.      ,\t False     ,\t False     ,\t []        ,\t \n",
      "startup   ,\t startup   ,\t NOUN      ,\t dep       ,\t xxxx      ,\t True      ,\t False     ,\t [for]     ,\t \n",
      "for       ,\t for       ,\t ADP       ,\t prep      ,\t xxx       ,\t True      ,\t True      ,\t [billion] ,\t \n",
      "$         ,\t $         ,\t SYM       ,\t quantmod  ,\t $         ,\t False     ,\t False     ,\t []        ,\t \n",
      "1         ,\t 1         ,\t NUM       ,\t compound  ,\t d         ,\t False     ,\t False     ,\t []        ,\t \n",
      "billion   ,\t billion   ,\t NUM       ,\t pobj      ,\t xxxx      ,\t True      ,\t False     ,\t [$, 1]    ,\t \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "fmt = \"{:10s},\\t \" * 8\n",
    "print(fmt.format('raw', 'stem', 'PartOfSpeech', 'dependency', 'shape', 'is alpha', 'is stop', 'its childrens in the parsing tree'))\n",
    "print('-'*140)\n",
    "for token in doc:\n",
    "    print(fmt.format(token.text, token.lemma_, token.pos_, token.dep_,\n",
    "            token.shape_, str(token.is_alpha), str(token.is_stop), str(list(token.children))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9y6cC-m5UJX"
   },
   "source": [
    "SpaCy also allows you to use the embeddings for both sentence and words\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1678610609505,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "HQXuIT-i5UJY",
    "outputId": "0c2b6639-827b-42bd-bef0-1ac161b62ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion [-0.49226812  0.40478638  0.5446301   0.2650897   0.5588461 ] ...\n",
      "Apple [-1.231103   -1.1917272   0.15840513  0.3598817   0.680532  ] ...\n",
      "is [-1.0020912  -0.24935524  0.2847814   0.7584369  -0.5807612 ] ...\n",
      "looking [-0.3423702  1.0666494  0.7334783  0.0921919 -1.0159137] ...\n",
      "at [ 1.4327683   1.9650179   0.528621   -1.103754   -0.29277676] ...\n",
      "buying [ 0.21374054  0.97006285 -0.37104425  0.25935912 -0.5281231 ] ...\n",
      "U.K. [-0.4769047 -0.6881261 -1.0802059  0.9870316  1.3138596] ...\n",
      "startup [-0.7687981   0.16186625  0.20556203 -0.70367986 -0.56370217] ...\n",
      "for [-0.12731966  0.20830332  1.3329215  -0.43356493 -0.7824546 ] ...\n",
      "$ [-0.5493651   1.2996746   0.19532208  0.46639207  1.8706077 ] ...\n",
      "1 [-1.1942618   0.7871655   5.5716734   0.19170347  3.1280797 ] ...\n",
      "billion [-1.3692445   0.12311826 -1.568583    2.0419886   2.91796   ] ...\n"
     ]
    }
   ],
   "source": [
    "print(doc, doc.vector[:5], '...')\n",
    "for t in doc:\n",
    "    print(t, t.vector[:5], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFBq_0Xe5UJY"
   },
   "source": [
    "For more usage of SpaCy, you can refer to its documentation at this link: https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75a7A16B5UJa"
   },
   "source": [
    "## 2. Baselines\n",
    "\n",
    "Finally, we provide two example baselines for your reference. The first baseline extracts TF-iDF features from texts and use logistic regression to generate prediction. The second baseline uses Convolutional Neural Networks (CNNs) to generate prediction from texts.\n",
    "\n",
    "\n",
    "We only consider its first 3k training samples. It is just an example, you can use the data as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpeYkRdXAOHl"
   },
   "source": [
    "### TF-IDF + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1678610733934,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "s8XXemtF5UJa"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1678610735689,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "N5AKPx2A5UJb",
    "outputId": "99c341c9-55bc-47d8-fa0d-341d9554c63f"
   },
   "outputs": [],
   "source": [
    "x_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "x_valid = valid_df['text']\n",
    "y_valid = valid_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1678610741896,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "NooKm1m75UJb",
    "outputId": "d4aa30ed-f95b-4ef8-dcf2-7c290c5751fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(tokenizer=<function tokenize at 0x000001DEF69F4220>)),\n",
      "                ('Truncated SVD', TruncatedSVD(n_components=500)),\n",
      "                ('lr', LogisticRegression(max_iter=1000, tol=0.005))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "lr = LogisticRegression(tol=5e-3,max_iter=1000)\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "steps = [('tfidf', tfidf),('Truncated SVD',svd),('lr', lr)]\n",
    "pipe = Pipeline(steps)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "executionInfo": {
     "elapsed": 19070,
     "status": "ok",
     "timestamp": 1678610762673,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "Xrav6WYa5UJb",
    "outputId": "69ff5979-9e95-4129-cafc-da279576040d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anacondom\\envs\\fl\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(tokenizer=&lt;function tokenize at 0x000001DEF69F4220&gt;)),\n",
       "                (&#x27;Truncated SVD&#x27;, TruncatedSVD(n_components=500)),\n",
       "                (&#x27;lr&#x27;, LogisticRegression(max_iter=1000, tol=0.005))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(tokenizer=&lt;function tokenize at 0x000001DEF69F4220&gt;)),\n",
       "                (&#x27;Truncated SVD&#x27;, TruncatedSVD(n_components=500)),\n",
       "                (&#x27;lr&#x27;, LogisticRegression(max_iter=1000, tol=0.005))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(tokenizer=&lt;function tokenize at 0x000001DEF69F4220&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=500)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, tol=0.005)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function tokenize at 0x000001DEF69F4220>)),\n",
       "                ('Truncated SVD', TruncatedSVD(n_components=500)),\n",
       "                ('lr', LogisticRegression(max_iter=1000, tol=0.005))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3697,
     "status": "ok",
     "timestamp": 1678610766310,
     "user": {
      "displayName": "Azir",
      "userId": "14152163960880447733"
     },
     "user_tz": -480
    },
    "id": "sjSUgqGJ5UJb",
    "outputId": "970fc723-6ff2-4086-db87-d50cb0a9b780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.53      0.55       295\n",
      "           2       0.40      0.13      0.19       198\n",
      "           3       0.46      0.58      0.51       508\n",
      "           4       0.46      0.42      0.44       523\n",
      "           5       0.58      0.68      0.63       476\n",
      "\n",
      "    accuracy                           0.51      2000\n",
      "   macro avg       0.50      0.47      0.47      2000\n",
      "weighted avg       0.50      0.51      0.50      2000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[156  17  70  23  29]\n",
      " [ 57  25  90  16  10]\n",
      " [ 26  19 294 120  49]\n",
      " [ 14   0 145 219 145]\n",
      " [ 15   1  38  97 325]]\n",
      "accuracy 0.5095\n",
      "                     id                                               text\n",
      "0   A3EMGD8RAEOK64_2907  On our trip this past summer to Lunenberg, Nov...\n",
      "1   A2BOWU2PX28BET_5501  Excellent!! Most remakes fall short of the ori...\n",
      "2  A100WO06OQR8BQ_10469  I started to watch this movie but it is such a...\n",
      "3  A2H4LKU7CPIUU9_11364  Well! I must be terribly jaded. Or I am comple...\n",
      "4  A14RF11JYGDKI8_23751  Dark and grim -- not a fun movie.  Watch it fo...\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe.predict(x_valid)\n",
    "\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('accuracy', np.mean(y_valid == y_pred))\n",
    "results = pd.DataFrame({'id': valid_df['id'], 'text': valid_df['text'], 'label': y_pred})\n",
    "results.to_csv('data/valid_pred.csv', index=False)\n",
    "test_pred = pipe.predict(test_df['text'])\n",
    "test_results = pd.DataFrame({'id': test_df['id'], 'text': test_df['text'], 'label': test_pred})\n",
    "test_results.to_csv('pred.csv', index=False)\n",
    "print(test_df.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
